{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'jars': [], 'name': 'session-vlad', 'numExecutors': 2, 'executorMemory': '1g', 'driverMemory': '3g', 'conf': {'spark.driver.maxResultSize': '2g', 'livy.server.yarn.app-lookup-timeout': '300s'}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>64</td><td>application_1533135836622_1290</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://smarty.azurehdinsight.net/yarnui/hn/proxy/application_1533135836622_1290/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn19-smarty.wuy0tdx3g3se3cgj52zzsduk3b.ax.internal.cloudapp.net:30060/node/containerlogs/container_e02_1533135836622_1290_01_000001/livy\">Link</a></td><td></td></tr><tr><td>65</td><td>application_1533135836622_1291</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://smarty.azurehdinsight.net/yarnui/hn/proxy/application_1533135836622_1291/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn31-smarty.wuy0tdx3g3se3cgj52zzsduk3b.ax.internal.cloudapp.net:30060/node/containerlogs/container_e02_1533135836622_1291_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{\n",
    " \"jars\": [],\n",
    " \"name\":\"session-vlad\",\n",
    " \"numExecutors\": 2,\n",
    " \"executorMemory\": \"1g\",\n",
    " \"driverMemory\": \"3g\",    \n",
    " \"conf\": {\n",
    "    \"spark.driver.maxResultSize\": \"2g\",\n",
    "    \"livy.server.yarn.app-lookup-timeout\": \"300s\"\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>67</td><td>application_1533135836622_1297</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://smarty.azurehdinsight.net/yarnui/hn/proxy/application_1533135836622_1297/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn35-smarty.wuy0tdx3g3se3cgj52zzsduk3b.ax.internal.cloudapp.net:30060/node/containerlogs/container_e02_1533135836622_1297_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3e74de8d"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.graphx.util.GraphGenerators"
     ]
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.graphx.util.GraphGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class Geohash"
     ]
    }
   ],
   "source": [
    "case class Geohash(gh: String, dist: Option[Double] = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph: org.apache.spark.graphx.Graph[Geohash,Double] = org.apache.spark.graphx.impl.GraphImpl@14a9045e"
     ]
    }
   ],
   "source": [
    "val eventsRdd: RDD[(VertexId, Geohash)] =\n",
    "  sc.parallelize(\n",
    "    Array((1L, Geohash(\"s\")), (2L, Geohash(\"t\")), \n",
    "          (3L, Geohash(\"y\")), (4L, Geohash(\"x\")), \n",
    "          (5L, Geohash(\"z\")))\n",
    "  )\n",
    "val relationshipsRdd: RDD[Edge[Double]] = \n",
    "    sc.parallelize(\n",
    "        Array(Edge(1L, 2L, 6.0), \n",
    "              Edge(1L, 3L, 7.0),\n",
    "              Edge(2L, 3L, 8.0), \n",
    "              Edge(2L, 4L, 5.0), \n",
    "              Edge(4L, 2L, -2.0),\n",
    "              Edge(2L, 5L, -4.0),\n",
    "              Edge(5L, 4L, 7.0),\n",
    "              Edge(5L, 1L, 2.0),\n",
    "              Edge(3L, 5L, 9.0),\n",
    "              Edge(3L, 4L, -3.0)                                        \n",
    "             )\n",
    "    )\n",
    "val graph: Graph[Geohash, Double] = Graph(eventsRdd, relationshipsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s  t 6.0\n",
      "s  y 7.0\n",
      "t  y 8.0\n",
      "t  x 5.0\n",
      "x  t -2.0\n",
      "t  z -4.0\n",
      "z  s 2.0\n",
      "z  x 7.0\n",
      "y  x -3.0\n",
      "y  z 9.0"
     ]
    }
   ],
   "source": [
    "val facts: RDD[String] =\n",
    "  graph.triplets.map(triplet =>\n",
    "   s\"${triplet.srcAttr.gh}  ${triplet.dstAttr.gh} ${triplet.attr}\")\n",
    "facts.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ vprog: (VertexId, VD, A) => VD$\n",
    "\n",
    "$ sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)]$\n",
    "\n",
    "$  mergeMsg: (A, A) => A $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vprog: (org.apache.spark.graphx.VertexId, Geohash, Double) => Geohash = <function3>"
     ]
    }
   ],
   "source": [
    "val sendMsg: (EdgeTriplet[Geohash, Double] => Iterator[(VertexId, Double)]) = { t =>     \n",
    "     (t.srcAttr.dist, t.dstAttr.dist) match {\n",
    "         case (Some(srcDist), None) => Iterator((t.dstId, srcDist + t.attr))\n",
    "         case (Some(srcDist), Some(targetDist)) if  targetDist > srcDist + t.attr => Iterator((t.dstId, srcDist + t.attr))\n",
    "         case _ => Iterator.empty                 \n",
    "     }       \n",
    "}\n",
    "\n",
    "val mergeMsg: ((Double, Double) => Double) = (x, y) => math.min(x, y)\n",
    "\n",
    "val vprog: ((VertexId, Geohash, Double) => Geohash) = (vid, gh, newDist) => {\n",
    "    gh.dist match { \n",
    "        case None => gh.copy(dist = Some(newDist))\n",
    "        case Some(d) if(d > newDist) => gh.copy(dist=Some(newDist))\n",
    "        case _ => gh\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s shortest dist: 0.0 \n",
      "t shortest dist: 2.0 \n",
      "y shortest dist: 7.0 \n",
      "x shortest dist: 4.0 \n",
      "z shortest dist: -2.0"
     ]
    }
   ],
   "source": [
    "val sourceId = 1L\n",
    "val initialGraph: Graph[Geohash, Double] = graph.mapVertices((id, x) => if (id == sourceId) Geohash(x.gh, Some(0.0)) else x)\n",
    "val ssp = initialGraph.pregel(initialMsg=Double.PositiveInfinity,\n",
    "                              maxIterations = 100000                        \n",
    "                             )(vprog, sendMsg, mergeMsg )\n",
    "\n",
    "ssp.vertices.map{\n",
    "    case (id, v) => s\"${v.gh} shortest dist: ${v.dist.get} \"}.collect.foreach(println(_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
